K8s 集群管理


1.TiDB-on-k8s

#使用过的部署方法：
     (1) TiDB官网提供的docker-compose文件转为k8s支持文件（利用kompose工具） ，由于k8s不支持本来动态分配存储空间。
     （2）使用TiDB提供的tidb-operator，由于其yaml文件有问题不能成功理由helm安装(yaml文件的语法错误)。官方团队也回复称现在这个工具还不太成熟，他们的计划就上TiDB在k8s上还需要二次开发。
    （3）使用git上tidb-on-k8s项目，由于版本原因，安装过程中一直存在bug,而且在修改yaml文件后其中也存在通信问题。由于通信问题需要改动k8s集群的配置，没有尝试。使用最新版本的镜像还会出现启动Tikv镜像报：超过系统最大文件打开数。

#采坑记录：
    （1）动态请求存储空间：TiDB部署在K8s上，一般采用外挂存储卷(PV)方式存储，而本测试中使用本地磁盘进行存储(local host)，k8s在目前1.9版本中不支持本地存储动态分配，导致创建TiDB时发出请求存储时（PVC）需要自己事先手动创建好存储空间（PV），如果不使用本地存储的话可以动态按需求分配。由于需要分配存储空间导致不能自动化。
    （2）版本问题：改申请存储空间为指定好存储空间，这样导致数据只能够在某台主机，不能方便操控数据。其中也存在问题，由于网上的例子的版本都是使用最新的，而现在过去很长时间，TiDB做出了一些更新，导致网上教程安装存在bug。
     （3）k8s通信问题：由于没有开启kube-dns服务，pd,tikv镜像启动后的etcd的地址无法解析。导致镜像不能正常运行。同时这个问题也存在与tensorflow分布式-on-k8s部署问题上。

2.tensorflow-on-k8s
#部署
     

#坑
    （1）通信问题：与TiDB一样都存在此问题。